\documentclass[a4paper, 12pt]{article}
\usepackage{comment} % enables the use of multi-line comments (\ifx \fi) 
\usepackage{lipsum} %This package just generates Lorem Ipsum filler text. 
\usepackage{fullpage} % changes the margin
\usepackage[a4paper, total={7in, 10in}]{geometry}
\usepackage{amsmath}
\usepackage{amssymb,amsthm}  % assumes amsmath package installed
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{quiver}
\usetikzlibrary{arrows}
\usepackage{verbatim}
\usepackage{setspace}
\usepackage{float}
\usepackage{tikz-cd}
\usepackage[backend=biber,bibencoding=utf8,style=numeric,sorting=ynt]{biblatex}

    
\usepackage{xcolor}
\usepackage{mdframed}
\usepackage[shortlabels]{enumitem}
\usepackage{indentfirst}
\usepackage{hyperref}
    
\renewcommand{\thesubsection}{\thesection.\alph{subsection}}

\newenvironment{problem}[2][Exercise]
    { \begin{mdframed}[backgroundcolor=gray!20] \textbf{#1 #2} \\}
    {  \end{mdframed}}

% Define solution environment
\newenvironment{solution}
    {\textit{Solution:}}
    {}

%Define the claim environment
\newenvironment{claim}[1]{\par\noindent\underline{Claim:}\space#1}{}
\newenvironment{claimproof}[1]{\par\noindent\underline{Proof:}\space#1}{\hfill $\blacksquare$}

\renewcommand{\qed}{\quad\qedsymbol}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
%Header-Make sure you update this information!!!!
\noindent
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\large\textbf{Zhengdong Zhang} \hfill \textbf{Homework - Week 6}   \\
Email: zhengz@uoregon.edu \hfill ID: 952091294 \\
\normalsize Course: MATH 647 - Abstract Algebra  \hfill Term: Fall 2024\\
Instructor: Dr.Victor Ostrik \hfill Due Date: $13^{th}$ November, 2024 \\
\noindent\rule{7in}{2.8pt}
\setstretch{1.1}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Exercise 4.2.6
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}{4.2.6}
For vector spaces \(U\) and \(V\), prove that \(S(U)\otimes S(V)\cong S(U\oplus V)\) as algebras. Is \(T(U)\otimes T(V)\cong T(U\oplus V)\)?
\end{problem}
\begin{solution}
Let \(X=\left\{ x_i \right\}_{i\in I}\) be a basis of \(U\) and 
\(Y=\left\{ y_j \right\}_{j\in J}\) be a basis of \(V\). By Corollary 4.2.4., \(\mathbb{F}[X]\) is uniquely isomorphic to \(S(U)\) as algebras and \(\mathbb{F}[Y]\) is uniquely isomorphic to \(S(V)\) as algebras. Moreover, 
since \(X\cup Y=\left\{ x_i,y_j \right\}_{i\in I,j\in J}\) is a basis for \(U\oplus Y\), so \(\mathbb{F}[X\cup Y]\) is uniquely isomorphic to \(S(U\oplus V)\) as \(\mathbb{F}\)-algebras. By Theorem 4.1.1., \(\left\{ x_i\otimes y_j \right\}_{i\in I,j\in J}\) is 
a basis for \(\mathbb{F}[X]\otimes \mathbb{F}[Y]\). We define a morphism 
\begin{align*}
	\phi:\mathbb{F}[X]\otimes \mathbb{F}[Y] & \rightarrow \mathbb{F}[X\cup Y],\\ 
	       x_i\otimes y_j &\mapsto x_iy_j
\end{align*}
for every \(i\in I\) and \(j\in J\). This map is obviously \(\mathbb{F}\)-linear. And we have 
\begin{align*}
	\phi((x_{i_1}\otimes y_{j_1})(x_{i_2}\otimes y_{j_2})) & = \phi(x_{i_1}x_{i_2}\otimes y_{j_1}y_{j_2})\\ 
	                                                       & =x_{i_1}x_{i_2}y_{j_1}y_{j_2}\\ 
														   & =x_{i_1}y_{j_1}x_{i_2}y_{j_2}\\ 
														   & =\phi(x_{i_1}\otimes y_{j_1})\phi(x_{i_2}\otimes y_{j_2})
\end{align*}
This shows that \(\phi\) is a morphism between \(\mathbb{F}\)-algebras. 
Recall that \(\mathbb{F}[X\cup Y]\) is generated by monomials \(p\) in \(X\cup Y\) and since it is commutative, we could rewrite the monomial \(p\) as \(p=fg\) where \(f\) is a monomial 
in \(X\) and \(g\) is a monomial in \(Y\). We could define a morphism 
\begin{align*}
	\psi: \mathbb{F}[X\cup Y] & \rightarrow \mathbb{F}[X]\otimes \mathbb{F}[Y],\\ 
	             p=fg&\mapsto f\otimes g
\end{align*}
This morphism is \(\mathbb{F}\)-linear and we have 
\begin{align*}
	\psi(p_1p_2) & =\psi(f_1g_1f_2g_2)\\ 
	             & =\psi(f_1f_2g_1g_2)\\ 
				 & =(f_1f_2)\otimes (g_1g_2)\\ 
				 & =(f_1\otimes g_1)(f_2\otimes g_2)\\ 
                 & =\psi(f_1g_1)\psi(f_2g_2)\\ 
				 & =\psi(p_1)\psi(p_2)
\end{align*}
This shows that \(\psi\) is a morphism between \(\mathbb{F}\)-algebras. Moreover, for each \(i\in I\) and \(j\in J\), we have \(\psi(\phi(x_i\otimes y_j))=\psi(x_iy_j)=x_i\otimes y_j\). So \(\psi\circ \phi=id\). 
And \(\phi(\psi(f_1g_1))=\phi(f_1\otimes g_1)=f_1g_1\), so \(\phi \circ \psi=id\). This proves that \(\phi\) and \(\psi\) are isomorphisms between \(\mathbb{F}[X]\otimes \mathbb{F}[Y]\) and \(\mathbb{F}[X\cup Y]\). Use the natural 
isomorphisms from Corollary 4.2.4. again, and we have \(S(U)\otimes S(V)\cong S(U\oplus V)\).
\par 
It is not true that \(T(U)\otimes T(V)\cong T(U\oplus V)\). Consider \(U\) and \(V\) are 1-dimensional vector spaces generated by \(x\) and \(y\) respectively. Then \(U\oplus V\) is a 2-dimensional vector space generated by \(\left\{ x,y \right\}\). We know 
that \(y\otimes x\) is an element in \(T(U\oplus V)\) but not an element in \(T(U)\otimes T(V)\), which, by Theorem 4.1.1., is generated by elements of the form \(p\otimes q\). Note that \(p\) is a pure tensor of basis in \(U\) and 
\(q\) is a pure tensor of basis in \(V\), so it cannot have the form \(y\otimes x\) as \(T(U)\otimes T(V)\) is not commutative. 
\end{solution}

\noindent\rule{7in}{2.8pt}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Exercise 4.2.11
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}{4.2.11}
Make \(\Gamma^n\) into a functor \(\Gamma^n:\mathbf{Vec}(F)\rightarrow \mathbf{Vec}(F)\) in such a way that \(s=(S_V)_{V\in \mathbf{Vec}(F)}\) defines a natural transformation 
\(s:S^n\Rightarrow \Gamma^n\). When is \(s\) an isomorphism of functors? 
\end{problem}
\begin{solution}
Let \(f:V\rightarrow W\) be a linear map between vector spaces. We first check that \(\Gamma^n\) is a functor by showing that \(\Gamma^n(f):\Gamma^n(V)\rightarrow \Gamma^n(W)\) is a well-defined linear map. We already have a linear map 
\(T^n(f):T^n(V)\rightarrow T^n(W)\) by functoriality of tensor power by sending the pure tensor \(v_1\otimes \cdots \otimes v_n\) to \(f(v_1)\otimes \cdots\otimes f(v_n)\) where \(v_1,v_2,\ldots,v_n\in V\). Write \(t=v_1\otimes \cdots\otimes v_n\) and note that 
for any \(g\in S_n\), we have 
\begin{align*}
	f(t)^g&=f(v_1\otimes \cdots\otimes v_n)^g\\ 
	      &=(f(v_1)\otimes \cdots\otimes f(v_n))^g\\ 
		  &=f(v_{g(1)})\otimes \cdots\otimes f(v_{g(n)})\\ 
		  &=f(v_{g(1)}\otimes \cdots\otimes f(v_g(n)))\\ 
		  &=f((v_1\otimes \cdots\otimes v_n)^g)\\ 
		  &=f(t^g)
\end{align*}
This shows that we have a well-defined linear map. It is obvious that \(\Gamma^n(id_V)=id_{\Gamma^n(V)}\). Suppose \(f_1:V\rightarrow W\) and \(f_2:W\rightarrow X\) are two linear maps and \(g\in S_n\), we have 
\((f_2\circ f_1)(t)^g=f_2(f_1(t)^g)=f_2(f_1(t^g))\) by our previous discussion and again use the functoriality of tensor power, we have \(\Gamma^n(f_2f_1)=\Gamma^n(f_2)\circ \Gamma^n(f_1)\). This proves that \(\Gamma^n\) is indeed a functor. 
\par 
Now we need to show that \(s:S^n\Rightarrow \Gamma^n\) is a natural transformation. Given a linear map \(f:V\rightarrow W\), let \(\left\{ v_i \right\}_{i\in I}\) be a basis of \(V\) and \(\leq\) is a total order on the index set \(I\), by Exercise 4.2.10., we know that 
\[\left\{ \text{sym}(v_{i_1},\ldots,v_{i_n})\,|\, i_1,\ldots,i_n\in I,i_1\leq \cdots\leq i_n \right\}\]
is a basis for \(\Gamma^n(V)\). And \(\Gamma^n(f)\) is defined by sending sym\((v_{i_1},\ldots,v_{i_n})\) to sym\((f(v_{i_1}),\ldots,f(v_{i_n}))\). Now we have a diagram:
\[\begin{tikzcd}
	{S^n(V)} & {\Gamma^n(V)} & {} \\
	{S^n(W)} & {\Gamma^n(W)}
	\arrow["{s_V}", from=1-1, to=1-2]
	\arrow["{S^n(f)}"', from=1-1, to=2-1]
	\arrow["{\Gamma^n(f)}", from=1-2, to=2-2]
	\arrow["{s_W}"', from=2-1, to=2-2]
\end{tikzcd}\]
To check that this diagram is indeed commutative, we have 
\begin{align*}
	(\Gamma^n(f)\circ s_V)(v_{i_1}\cdots v_{i_n}) & =\Gamma^n(f)(\prod_{k\in I}m_k!)\text{sym}(v_{i_1},\ldots,v_{i_n})\\ 
	                                             & =(\prod_{k\in I}m_k!)\text{sym}(f(v_{i_1}),\ldots,f(v_{i_n}))\\ 
												 & =s_W(f(v_{i_1}),\ldots,f(v_{i_n}))\\ 
												 & =(s_W\circ S^n(f))(v_{i_1}\cdots v_{i_n})
\end{align*}
This shows that \(s\) is a natural transformation.
\par 
We know that for a vector space \(V\), \(s_V:S^n(V)\xrightarrow{\sim} \Gamma^n(V)\) is an isomorphism if and only if char\(\mathbb{F}=0\) or char\(\mathbb{F}>n\). By Exercise 2.4.2., we know that a natural transformation is an isomorphism of functors if and only if it is an isomorphism on each object. 
So \(s\) is an isomorphism of functors if and only if char\(\mathbb{F}=0\) or char\(\mathbb{F}>n\). 
\end{solution}

\noindent\rule{7in}{2.8pt}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Exercise 4.3.1
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}{4.3.1}
Assuming that char\(\mathbb{F}\neq 2\), show that the ideal \(J\) just defined is equal to the twoo-sided ideal of the tensor algebra generated by the elements \(\left\{ v\otimes w+w\otimes v\,|\, v,w\in V \right\}\). 
(This is not true if char\(\mathbb{F}=2\).)
\end{problem}
\begin{solution}
Write \(J'\) as the two-sided ideal in \(T(V)\) generated by the elements \(\left\{ v\otimes w+w\otimes v\,|\, v,w\in V \right\}\). Because char\(\mathbb{F}\neq 2\), for any \(v,w\in V\), we have 
\[(v+w)\otimes (v+w)-v\otimes v-w\otimes w=v\otimes w+w\otimes v.\]
So \(J'\subset J\). On the other hand, we have \(2\neq 0\) in \(\mathbb{F}\), so for each \(v\in V\), 
\[v\otimes v=\frac{1}{2}(v\otimes v+v\otimes v).\]
So \(J\subset J'\) and we conclude that \(J=J'\).
\end{solution}

\noindent\rule{7in}{2.8pt}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Exercise 4.3.7
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}{4.3.7}
Suppose that \(V\) is finite dimensional of dimension \(m\). Show that \(\dim \wedge^n(V)=\binom{m}{n}\) and that \(\dim \wedge(V)=2^m\).	
\end{problem}
\begin{solution}
Let \(x_1,x_2,\ldots,x_m\) be a basis of \(V\) with a total order \(x_1\leq x_2\leq \cdots\leq x_m\). By Theorem 4.3.6., the set 
\[\left\{ x_{i_1}\wedge \cdots\wedge x_{i_n}\,|\, 1\leq i_1<i_2<\cdots<i_n\leq m \right\}\]
is a basis of \(\wedge^n(V)\). Namely we have to choose \(n\) elements from the set \(\left\{ x_1,\ldots, x_m \right\}\)(order does not matter). So we have \(\binom{m}{n}\) different choices and \(\dim \wedge^n(V)=\binom{m}{n}\). 
Moreover, \(\wedge(V)=\bigoplus_{n=0}^m \wedge^n(V)\) implies that 
\[\dim \wedge(V)=\bigoplus_{n=0}^m \dim \wedge^n(V)=\sum_{n=0}^{m}\binom{m}{n}=(1+1)^m=2^m.\]
\end{solution}

\noindent\rule{7in}{2.8pt}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Exercise 4.3.8
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}{4.3.8}
Let \(V\) and \(W\) be finite dimensional vector spaces. Show that \(\wedge (V\oplus W)\) and \(\wedge (V)\otimes \wedge (W)\) are isomorphic as vector spaces. Give a 
counterexample to show that they need not be isomorphic as algebras.
\end{problem}
\begin{solution}
Let \(x_1,\ldots,x_n\) be a basis of \(V\) with a total order \(x_1\leq \cdots\leq x_n\) and \(y_1,\ldots,y_m\) a basis of \(W\) with a total order \(y_1\leq \cdots\leq y_m\). Then \(x_1,\ldots,x_n,y_1,\ldots,y_m\) is a basis of 
\(V\oplus W\) with a total order \(x_1\leq \cdots\leq x_n\leq y_1\leq \cdots\leq y_m\). Note that both \(\wedge(V\oplus W)=\bigoplus_{i=0}^{m+n} \wedge^i(V)\) and \(\wedge(V)\otimes \wedge(W)=(\bigoplus_{i=0}^n\wedge^i(V))\otimes (\bigoplus_{j=0}^m) \wedge^j(W)\) are 
finite dimensional as vector spaces, so to prove they are isomorphic as vector spaces, we only need to show that they have the same dimension. By Exercise 4.3.7., \(\dim \wedge (V)=2^n\) and \(\dim \wedge (W)=2^m\), moreover, by Theorem 4.1.1., 
\[\dim (\wedge(V)\otimes \wedge(W))=\dim \wedge(V)\times \dim \wedge(W)=2^n\cdot 2^m=2^{n+m}=\dim \wedge(V\oplus W).\]
\par 
To see that they are not isomorphic as \(\mathbb{F}\)-algebras, let \(n=m=2\). We have 
\begin{align*}
	(x_1\wedge y_1)\cdot(x_2\wedge y_2) & =x_1\wedge y_1\wedge x_2\wedge y_2\\ 
	                                    & =-x_1\wedge x_2\wedge y_1\wedge y_2
\end{align*}
in \(\wedge (V\oplus W)\). But in \(\wedge(V)\otimes \wedge(W)\), we have \((x_1\otimes y_1)\cdot(x_2\otimes y_2)=(x_1\wedge x_2)\otimes (y_1\wedge y_2)\). They differ by a sign, so we do not have an 
\(\mathbb{F}\)-algebra isomorphism.
\end{solution}

\noindent\rule{7in}{2.8pt}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Exercise 4.3.15
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}{4.3.15}
Show that there is a well-defined, injective linear map 
\begin{align*}
	a_V:\wedge^n(V) &\rightarrow T^n(V),\\ 
	  v_1\wedge \cdots \wedge v_n &\mapsto \sum_{g\in S_n} \text{sgn}(g)v_{g(1)}\otimes \cdots \otimes v_{g(n)}.
\end{align*}
This defines a natural isomorphism between \(\wedge^n(V)\) and the subspace of \(T^n(V)\) consisting of all \textit{anti-symmetric tensors}.
\end{problem}
\begin{solution}
The map \(a_V\) is obviously linear. Now we are showing that it is injective. Let \(v\in \ker a_V\) satisfying \(a_V(v)=0\). We first observe that \(v\) must be a pure tensor. Suppose \(v\) is the linear combination of 
at least two basis \(v_1\wedge \cdots\wedge v_n\) and \(w_1\wedge \cdots\wedge w_n\). In the exterior algebra we cannot have \(\left\{ v_1,\ldots,v_n \right\}=\left\{ w_1,\ldots,w_n \right\}\), otherwise 
\[v_1\wedge \cdots\wedge v_n=\text{sgn}(g)w_{g(1)}\wedge \cdots\wedge w_{g(n)}\]
for some \(g\in S_n\), which contradicts that they are linearly independent. So \(v\) must be a pure tensor \(v=v_1\wedge v_2\wedge \cdots\wedge v_n\). We know that 
\[0=a_V(v)=\sum_{g\in S_n}\text{sgn}(g)v_{g(1)}\otimes \cdots\otimes v_{g(n)}.\]
For different \(g,h\in S_n\), \(v_{g(1)}\otimes \cdots\otimes v_{g(n)}\) and \(v_{h(1)}\otimes \cdots\otimes v_{h(n)}\) are linearly independent because two different elements in \(S_n\) represents different permutations. And for any \(g\in S_n\), 
\(\text{sgn}(g)\) can only be \(\pm 1\). So for any \(g\in S_n\), \(v_{g(1)}\wedge \cdots\wedge v_{g(n)}=0\). This implies \(v=0\). Thus we can conclude that \(a_V\) is injective because \(\ker a_V=0\). 
\par 
Next we are going to show that \(a\) defines a natural transformation \(a:\wedge^n\Rightarrow T^n\). Let \(f:V\rightarrow W\) be a linear map. We have a diagram:
\[\begin{tikzcd}
	{\wedge^n(V)} & {T^n(V)} & {} \\
	{\wedge^n(W)} & {T^n(W)}
	\arrow["{a_V}", from=1-1, to=1-2]
	\arrow["{\wedge^n(f)}"', from=1-1, to=2-1]
	\arrow["{T^n(f)}", from=1-2, to=2-2]
	\arrow["{a_W}"', from=2-1, to=2-2]
\end{tikzcd}\]
To see this diagram commutes, given \(v_1\wedge \cdots\wedge v_n\in \wedge^n(V)\) ,we have 
\begin{align*}
	(T^n(f)\circ a_V)(v_1\wedge \cdots\wedge v_n) & =T^n(f)(\sum_{g\in S_n}\text{sgn}v_{g(1)}\otimes \cdots\otimes v_{g(n)})\\ 
	                                              & =\sum_{g\in S_n}f(v_{g(1)})\otimes \cdots\otimes f(v_{g(n)})\\ 
												  & =a_W(f(v_1)\wedge \cdots\wedge f(v_n))\\ 
												  & =(a_W\circ \wedge^n(f))(v_1\wedge \cdots\wedge v_n)
\end{align*}
This gives us a natural isomorphism between \(\wedge^n(V)\) and the subspace of \(T^n(V)\) consisting of all \textit{anti-symmetric tensors}.
\end{solution}

\noindent\rule{7in}{2.8pt}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Exercise 4.3.16
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}{4.3.16}
Let \(V\) be a finite dimensional vector space. Construct a natural isomorphism between \(\wedge^n(V^*)\) and \(\wedge^n(V)^*\).
\end{problem}
\begin{solution}
Let \(v_1,\ldots,v_m\) be a basis of \(V\) with a total order \(v_1\leq \cdots\leq v_m\), so \(v_1^*,\ldots,v_m^*\) is a basis for \(V^*\) with a total order \(v_1^*\leq \cdots\leq v_m^*\). We know by Theorem 4.3.6. that 
\[\left\{ v_{i_1}\wedge \cdots\wedge v_{i_n}\,|\, 1\leq i_1< i_2<\cdots<i_n\leq m \right\}\]
is a basis of \(\wedge^n(V)\) and 
\[\left\{ v_{i_1}^*\wedge \cdots\wedge v_{i_n}^*\,|\, 1\leq i_1< i_2<\cdots<i_n\leq m \right\}\]
is a basis of \(\wedge^n(V^*)\). We define a map 
\begin{align*}
	\alpha_V:\wedge^n(V^*) \rightarrow & \wedge^n(V)^*,\\ 
	       v_{i_1}^*\wedge\cdots\wedge v_{i_n}^*\mapsto & (v_{i_1}\wedge\cdots\wedge v_{i_n})^*
\end{align*}
This map is linear by definition. Moreover, we know that \((\wedge^n(V))^*\) has a basis by taking the dual basis of \(\wedge^n(V)\), so we can define a inverse map 
\begin{align*}
	\alpha^{-1}_V:\wedge^n(V)^*&\rightarrow \wedge^n(V^*),\\
	(v_{i_1}\wedge\cdots\wedge v_{i_n})^* &\mapsto v_{i_1}^*\wedge\cdots\wedge v_{i_n}^*
\end{align*}
We only need to show that \(\alpha\) is natural. Let \(f:V\rightarrow W\) be a linear map. We have a diagram:
\[\begin{tikzcd}
	{\wedge^n(W^*)} & {\wedge^n(W)^*} & {} \\
	{\wedge^n(V^*)} & {\wedge^n(V)^*}
	\arrow["{\alpha_W}", from=1-1, to=1-2]
	\arrow["{\wedge^n(f^*)}"', from=1-1, to=2-1]
	\arrow["{\wedge^n(f)^*}", from=1-2, to=2-2]
	\arrow["{\alpha_V}"', from=2-1, to=2-2]
\end{tikzcd}\]
Let \(w_1,\ldots,w_k\) be a basis of \(W\) with a total order \(w_1\leq \cdots\leq w_k\). To see this diagram indeed commutes, we have 
\begin{align*}
	(\wedge^n(f)^*\circ \alpha_W)(w_{i_1}^*\wedge \cdots\wedge w_{i_n}^*) & =\wedge^n(f)^*(w_{i_1}\wedge\cdots\wedge w_{i_n})^*\\ 
	                                                              & =(f(w_{i_1})\wedge\cdots\wedge f(w_{i_n}))^*\\ 
																  & =\alpha_V(f(w_{i_1})^*\wedge\cdots\wedge f(w_{i_n})^*)\\ 
																  & =(\alpha_V\circ \wedge^n(f^*))(w_{i_1}\wedge \cdots\wedge w_{i_n})
\end{align*}
\end{solution}

\noindent\rule{7in}{2.8pt}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Exercise 4.4.2
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}{4.4.2}
Let \(A\) be the Gram matrix of a bilinear form \((\cdot|\cdot)\) on \(V\) with respect to basis \(v_1,\ldots,v_n\).
\begin{enumerate}[(1)]
\item Suppose that \(v\) and \(w\) are vectors in \(V\) represented as column vectors \([v]\) and \([w]\) with respect to the given basis. Show that 
       \[(v|w)=[v]^TA[w].\]
\item Let \(v_1',\ldots, v_n'\) be another basis for \(V\) and \(A'\) be the corresponding Gram matrix. Show that 
       \[A=P^TA'P \]
	   where \(P=[p_{i,j}]_{1\leq i,j\leq n}\) is the change of basis matrix from \(v_1,\ldots,v_n\) to \(v_1'.\ldots,v_n'\) defined from \(v_j=\sum_{i=1}^{n}p_{i,j}v_i'\).
\item Show that the bilinear form is non-degenerate if and only if its Gram matrix \(A\) is invertible.
\end{enumerate}
\end{problem}
\begin{solution}
	
\end{solution}
\begin{enumerate}[(1)]
\item Write \(v=\sum_{i=1}^{n}c_jv_j\) and \(w=\sum_{j=1}^{n}d_iv_i\), so \([v]=\begin{pmatrix}
	c_1\\ 
	c_2\\ 
	\vdots\\ 
	c_n
\end{pmatrix}\) and \(w=\begin{pmatrix}
	d_1\\
	d_2\\
	\vdots\\ 
	d_n
\end{pmatrix}\). Therefore, we have
\[[v]^T A [w]=\begin{pmatrix}
	\sum_{i=1}^{n} c_ia_{i,1} & \sum_{i=1}^{n}c_i a_{i,2} &\cdots& \sum_{i=1}^{n}c_ia_{i,n}
\end{pmatrix}\begin{pmatrix}
	d_1\\
	d_2\\
	\vdots\\
	d_n
\end{pmatrix}=\sum_{i=1}^{n}\sum_{j=1}^{n}c_id_ja_{i,j}.\]
On the other hand, 
\[(v|w)=(\sum_{i=1}^{n}c_iv_i)|\sum_{j=1}^{n}d_jv_j=\sum_{i=1}^{n}\sum_{j=1}^{n}c_id_j(v_i|v_j)=\sum_{i=1}^{n}\sum_{j=1}^{n}c_id_ja_{i,j}=[v]^T A[w].\]
\item For each \(1\leq i,j\leq n\), we have 
\begin{align*}
	a_{i,j} & =(v_i|v_j)\\ 
	        & =(\sum_{k=1}^{n}p_{k,i}v_k'|\sum_{l=1}^{n}p_{l,j}v_l')\\ 
			& =\begin{pmatrix}
				p_{1,i}&p_{2,i}&\cdots&p_{n,i}
			\end{pmatrix}\begin{pmatrix}
				(v_1'|v_1') & (v_1'|v_2') &\cdots & (v_1'|v_n')\\ 
				(v_2'|v_1') & (v_2'|v_2') &\cdots & (v_2'|v_n')\\ 
				\vdots &\vdots &\ddots & \vdots \\ 
				(v_n'|v_1') & (v_n'|v_2') &\cdots & (v_n'|v_n')
			\end{pmatrix}\begin{pmatrix}
				p_{1,j}\\ 
				p_{2,j}\\ 
				\vdots\\ 
				p_{n,j}
			\end{pmatrix}\\ 
			&=\begin{pmatrix}
				p_{1,i}&p_{2,i}&\cdots&p_{n,i}
			\end{pmatrix}A'\begin{pmatrix}
				p_{1,j}\\ 
				p_{2,j}\\ 
				\vdots\\ 
				p_{n,j}
			\end{pmatrix}
\end{align*}
Varying \(i\) and \(j\) and we have \(A=P^TA'P\).
\item We prove this by showing that \(A\) is the matrix representation for the linear map \(\beta:V\rightarrow V^*\) induced by the bilinear form \((-|-)\). Then by Lemma 4.4.1., \(A\) is invertible if and only if 
the induced map \(\beta:V\rightarrow V^*\) is an isomorphism if and only if the bilinear form is non-degenerate. We know that for any \(v\in V\), \(\beta(v)=(v|-)\in V^*\) by definition. Let \(v_1^*,\ldots,v_n^*\) be a basis 
of \(V^*\). For each \(1\leq i,j\leq n\), we know that the \((i,j)\)-entry of the matrix related to the linear map \(\beta:V\rightarrow V^*\) is given by \((v_j^*)^*(\beta(v_i))=\beta(v_i)(v_j)=(v_i|v_j)\). This proves our claim.
\end{enumerate}
\noindent\rule{7in}{2.8pt}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Exercise 4.4.11
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}{4.4.11}
Let \(V\) be a finite dimensional vector space equipped with a skew-symmetric bilinear form. Prove that every maximal isotropic subspace of \(V\) has the same dimension, and express 
this dimension in terms of \(\dim V\) and the integer \(k\) from Theorem 4.4.6.
\end{problem}
\begin{solution}
First we assume the bilinear form \((-|-)\) is non-degenerate. We are going to show that the dimension of a maximal isotropic subspace is equal to \(\frac{1}{2}\dim V\). Let \(U\) be a maximal isotropic subspace of \(V\). By Lemma 4.4.5, we have 
\[\dim V=\dim U+\dim U^\perp\geq 2\dim U.\]
So \(\dim U\leq \frac{1}{2}\dim V\). We prove the equality can be achieved by induction on the dimension \(\dim V=2n\)(By Corollary 4.4.7, \((-|-)\) is non-degenerate implies that \(\dim V\) is even). When \(n=1\), there exists a basis \(v_1,v_2\) of \(V\) such that the Gram 
matrix can be written as \(A=\begin{pmatrix}
	0 & 1\\ 
	-1 &0
\end{pmatrix}\). Consider the subspace \(U\) generated by \(v_1\). We know that \(U\) is isotropic since \((v_1|v_1)=0\) and \(\dim U=\frac{1}{2}\dim V=1\). Now assume \(n\geq 2\) we have proven the case \(n-1\), we prove the case \(\dim V=2n\). There exists an isotropic subspace \(U\) of dimension \(n-1\) 
in \(V\) with a basis \(u_1,u_2,\ldots,u_{n-1}\). By Exercise 4.4.9, there exists an isotropic subspace \(U'\) with basis \(u_1',u_2',\ldots,u_{n-1}'\) such that \(U\cap U'=0\) and \((u_i|u_j')=\delta_{i,j}\) for all \(1\leq i,j\leq n-1\). Then \(U+ U'\) is a subspace of dimension 
\(2n-2\) by Exercise 1.2.4., and by the Steinitz Exchange Lemma, the basis \(\left\{ u_1,\ldots,u_{n-1},u_1',\ldots,u_{n-1}' \right\}\) of \(U+U'\) can be extended to a basis 
\(\left\{ u_1,\ldots,u_{n-1},u_1',\ldots,u_{n-1}',v_1.v_2 \right\}\) of \(V\). By Lemma 4.4.5, \(\dim U^\perp=\dim V-\dim U=n+1\), note that for each \(i=1,2,\ldots,n-1\), we have \((u_i|u_i')=1\), so \(U'\cap U^\perp=0\). So 
\(U^\perp=\text{span}(u_1,u_2,\ldots,u_{n-1},v_1,v_2)\). Let \(W=\text{span}(u_1,u_2,\ldots,u_{n-1},v_1)\). We have \((v_1|v_1)=0\) because \((-|-)\) is skew-symmetric and \((u_i|v_1)=0\) for all \(1\leq i\leq n-1\) because \(v_1\in U^\perp\). This shows that \(W\) is an isotropic subspace of \(V\) with \(\dim W=n=\frac{1}{2}\dim V\).
\par 
Now we drop the assumption that \((-|-)\) is non-degenerate. By Theorem 4.4.6, there exists a basis \(v_1,\ldots,v_n\) such that \(V\) can be written as \(V=V_1\oplus V_2\) where \(V_1\) has the corresponding Gram matrix which is \(k\) block matrices \(\begin{pmatrix}
	0 & 1\\ 
	-1 & 0
\end{pmatrix}\) and \(V_2\) corresponding to the zero matrix. \(\dim V_1\) and \(\dim V_2\) is invariant under base change and we know that \((-|-)\) restricting to \(V_1\) is non-degenerate since the Gram matrix for \(V_1\) is invertible. So the dimension of a maximal isotropic subspace \(U\) in \(V_1\) is always \(\frac{1}{2}\dim V_1=\frac{1}{2}\cdot 2k=k\). Note 
that for any \(v_2\in V_2\), we have \((v_2|v_i)=(v_i|v_2)=0\) from the Gram matrix. Thus, a maximal isotropic subspace of \(V\) can always be written as \(U\oplus V_2\) where \(U\) is a maximal isotropic subspace of \(V_1\), and \(\dim (U\oplus V_2)=\dim V-k\) is invariant under base change. 	
\end{solution}

\noindent\rule{7in}{2.8pt}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Exercise 4.4.14
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}{4.4.14}
Let \(V\) be a finite dimensional real vector space equipped with a symmetric bilinear form. Show that every subspace \(U\) of \(V\) is non-degenerate if and only if 
the form is positive or negative definite.
\end{problem}
\begin{solution}
Let \((p,m,z)\) be the signature of this real symmetric bilinear form. We know \((-|-)\) is non-degenerate on \(V\) if and only if \(z=0\). So we can assume \(z=0\). Now if \(p,m\geq 1\), then by Exercise 4.4.13, there exists a basis 
\(v_1,\ldots,v_n\) for \(V\) such that the Gram matrix has the form  
\[A=\begin{pmatrix}
	1 & & & & &\\
	  &\ddots & & & &\\ 
	  &  & 1 & & &\\ 
      & & & -1 & &\\ 
	  & & & &\ddots & \\ 
	  & & & & & -1
\end{pmatrix}\]
Consider the \(1\)-dimensional subspace \(U\) of \(V\) generated by \(v_p+v_{p+1}\). We know that 
\begin{align*}
	(v_p+v_{p+1}|v_p+v_{p+1}) & =(v_p|v_p)+(v_{p+1}|v_{p+1})+2(v_p|v_{p+1})\\ 
	                          & =1+(-1)+2\cdot 0\\ 
							  & =0
\end{align*}
So \(U\) is a degenerate subspace. Thus, if every subspace \(U\) of \(V\) is non-degenerate, then the signature must have \(p=z=0\) or \(m=z=0\), which is equivalent to \((-|-)\) is positive definite or negative definite. 
\par 
Conversely, assume the bilinear form \((-|-)\) is positive definite or negative definite. Then by Exercise 4.4.13., there exists a basis \(v_1,\ldots,v_n\) such that the corresponding Gram matrix \(A=I_n\) or \(A=-I_n\). We prove that every subspace \(U\) 
of \(V\) is non-degenerate by showing that \((v|v)=0\) implies \(v=0\). Write \(v\) in the basis as \(v=\sum_{i=1}^{n}a_iv_i\). We have 
\begin{align*}
	0 & =(v|v)\\ 
	  & =(\sum_{i=1}^{n}a_iv_i|\sum_{j=1}^{n}a_jv_j)\\ 
	  & =\sum_{i=1}^{n}a_i^2(v_i|v_i)\\ 
	  & =\sum_{i=1}^{n}a_i^2
\end{align*}
This shows that \(a_i=0\) for \(i=1,2,\ldots,n\).
\end{solution}
\end{document}